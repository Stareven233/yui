{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-0864_dB5Mv",
        "outputId": "ebff858c-b8ef-4365-d58a-1c86fb48cd14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.5 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n",
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:          12986         579       10429           1        1977       12175\n",
            "Swap:             0           0           0\n",
            "Sat Apr 30 05:56:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P8    34W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-22.0.4\n",
            "Collecting mir_eval\n",
            "  Downloading mir_eval-0.7.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 KB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.4.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mir_eval) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from mir_eval) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.8)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.10.8)\n",
            "Building wheels for collected packages: mir_eval\n",
            "  Building wheel for mir_eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mir_eval: filename=mir_eval-0.7-py3-none-any.whl size=100721 sha256=7f987f60c1616d4f198ce9e63c45087fcaaa2001a515834c7393b7702adab0ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/5a/46/d2527ff1fd975e1a793375e6ed763bfe4d3ea396b7cdc470eb\n",
            "Successfully built mir_eval\n",
            "Installing collected packages: mir_eval\n",
            "Successfully installed mir_eval-0.7\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting note_seq==0.0.3\n",
            "  Downloading note_seq-0.0.3-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.1/210.1 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: intervaltree>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (1.4.1)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (21.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (1.0.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (5.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (1.21.6)\n",
            "Collecting pretty-midi>=0.2.6\n",
            "  Downloading pretty_midi-0.2.9.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (3.17.3)\n",
            "Requirement already satisfied: bokeh>=0.12.0 in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (2.3.3)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: librosa>=0.6.2 in /usr/local/lib/python3.7/dist-packages (from note_seq==0.0.3) (0.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m57.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note_seq==0.0.3) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note_seq==0.0.3) (2.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note_seq==0.0.3) (4.2.0)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh>=0.12.0->note_seq==0.0.3) (5.1.1)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from intervaltree>=2.1.0->note_seq==0.0.3) (2.4.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->note_seq==0.0.3) (1.6.0)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->note_seq==0.0.3) (0.2.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->note_seq==0.0.3) (0.10.3.post1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->note_seq==0.0.3) (2.1.9)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->note_seq==0.0.3) (0.51.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.6.2->note_seq==0.0.3) (4.4.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Collecting mido>=1.1.16\n",
            "  Downloading mido-1.2.10-py2.py3-none-any.whl (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from pretty-midi>=0.2.6->note_seq==0.0.3) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->note_seq==0.0.3) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->note_seq==0.0.3) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->note_seq==0.0.3) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->note_seq==0.0.3) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->note_seq==0.0.3) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->note_seq==0.0.3) (5.1.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->note_seq==0.0.3) (57.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh>=0.12.0->note_seq==0.0.3) (2.0.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa>=0.6.2->note_seq==0.0.3) (0.34.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.6.2->note_seq==0.0.3) (1.4.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->note_seq==0.0.3) (0.2.5)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.6.2->note_seq==0.0.3) (1.15.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->note_seq==0.0.3) (0.7.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.6.2->note_seq==0.0.3) (2.21)\n",
            "Building wheels for collected packages: pretty-midi\n",
            "  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty-midi: filename=pretty_midi-0.2.9-py3-none-any.whl size=5591955 sha256=c723dc90124101b0abea13fde8385b4b55e94c17c0509ecbba281385011ac1fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/74/7c/a06473ca8dcb63efb98c1e67667ce39d52100f837835ea18fa\n",
            "Successfully built pretty-midi\n",
            "Installing collected packages: tokenizers, pydub, mido, sacremoses, pyyaml, pretty-midi, huggingface-hub, transformers, note_seq\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 mido-1.2.10 note_seq-0.0.3 pretty-midi-0.2.9 pydub-0.25.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!lsb_release -a\n",
        "# !cat /etc/shells\n",
        "# !echo $SHELL\n",
        "!cat /proc/cpuinfo \n",
        "!free -m\n",
        "!nvidia-smi\n",
        "\n",
        "%pip install --upgrade pip\n",
        "%pip install mir_eval librosa h5py\n",
        "# %pip install torch==1.10.2 torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "%pip install note_seq==0.0.3 transformers  scikit-learn  pandas\n",
        "\n",
        "# !git clone https://github.com/NVIDIA/apex\n",
        "# %pip install -v --disable-pip-version-check --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" /content/apex/\n",
        "# # 使用 apex.normalization.FusedRMSNorm 替代 T5LayerNorm 加快计算\n",
        "# 安装要将近20分钟，还是算了吧"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNq5APb7EIHR",
        "outputId": "708475f7-b7a4-4870-deb4-f4c37a43671b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "env: KAGGLE_CONFIG_DIR=/content/drive/MyDrive/kaggle\n",
            "Downloading maestrov300-hdf5.zip to /content\n",
            "100% 18.0G/18.0G [06:32<00:00, 76.9MB/s]\n",
            "100% 18.0G/18.0G [06:32<00:00, 49.1MB/s]\n",
            "rm: cannot remove '/content//yui_py37/': No such file or directory\n",
            "Archive:  /content/drive/MyDrive/yui_py37.zip\n",
            "  inflating: /content/yui_py37/__init__.py  \n",
            "   creating: /content/yui_py37/__pycache__/\n",
            "  inflating: /content/yui_py37/__pycache__/datasets.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/__pycache__/event_codec.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/__pycache__/note_sequences.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/__pycache__/preprocessors.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/__pycache__/utils.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/__pycache__/vocabularies.cpython-39.pyc  \n",
            "   creating: /content/yui_py37/config/\n",
            "  inflating: /content/yui_py37/config/__init__.py  \n",
            "   creating: /content/yui_py37/config/__pycache__/\n",
            "  inflating: /content/yui_py37/config/__pycache__/__init__.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/config/__pycache__/data.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/config/__pycache__/model.cpython-39.pyc  \n",
            "  inflating: /content/yui_py37/config/data.py  \n",
            "  inflating: /content/yui_py37/config/model.py  \n",
            "  inflating: /content/yui_py37/datasets.py  \n",
            "  inflating: /content/yui_py37/evaluate.py  \n",
            "  inflating: /content/yui_py37/event_codec.py  \n",
            "  inflating: /content/yui_py37/note_sequences.py  \n",
            "   creating: /content/yui_py37/notebook/\n",
            "  inflating: /content/yui_py37/notebook/keep_training.py  \n",
            "  inflating: /content/yui_py37/notebook/yui_py37_colab.ipynb  \n",
            "  inflating: /content/yui_py37/notebook/yui_py37_kaggle.ipynb  \n",
            "  inflating: /content/yui_py37/notebook/yui-kaggle.ipynb  \n",
            "  inflating: /content/yui_py37/postprocessors.py  \n",
            "  inflating: /content/yui_py37/preprocessors.py  \n",
            "  inflating: /content/yui_py37/test.py  \n",
            "  inflating: /content/yui_py37/train.py  \n",
            "  inflating: /content/yui_py37/try.py  \n",
            "  inflating: /content/yui_py37/utils.py  \n",
            "  inflating: /content/yui_py37/vocabularies.py  \n",
            "Archive:  /content/maestrov300-hdf5.zip\n",
            "  inflating: /content/maestro/2004.h5  \n",
            "  inflating: /content/maestro/2006.h5  \n",
            "  inflating: /content/maestro/2008.h5  \n",
            "  inflating: /content/maestro/2009.h5  \n",
            "  inflating: /content/maestro/2011.h5  \n",
            "  inflating: /content/maestro/2013.h5  \n",
            "  inflating: /content/maestro/2014.h5  \n",
            "  inflating: /content/maestro/2015.h5  \n",
            "  inflating: /content/maestro/2017.h5  \n",
            "  inflating: /content/maestro/2018.h5  \n",
            "  inflating: /content/maestro/LICENSE  \n",
            "  inflating: /content/maestro/README  \n",
            "  inflating: /content/maestro/maestro-v3.0.0.csv  \n",
            "  inflating: /content/maestro/maestro-v3.0.0.json  \n",
            "  inflating: /content/maestro/maestro-v3.0.0_tinymp3.csv  \n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%set_env KAGGLE_CONFIG_DIR=/content/drive/MyDrive/kaggle\n",
        "# 替代export设置kaggle文件夹的位置，路径不能有引号\n",
        "!kaggle datasets download -d stareven233/maestrov300-hdf5\n",
        "\n",
        "!rm /content//yui_py37/ -r\n",
        "!unzip /content/drive/MyDrive/yui_py37.zip -d /content/\n",
        "sys.path.insert(0, r'/content/yui_py37')\n",
        "!unzip /content/maestrov300-hdf5.zip -d /content/maestro/\n",
        "!rm /content/maestrov300-hdf5.zip\n",
        "\n",
        "# !unzip /content/drive/MyDrive/datasets/maestrov200.zip -d /content/maestro/\n",
        "# !unzip /content/drive/MyDrive/checkpoints.zip -d /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i2eTb9HB5M0",
        "outputId": "79fef463-2d01-413c-d0ac-1d5fda7e7e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA GPUs are available\n",
            "number of avaliable gpu: 1\n",
            "index of current device: 0\n",
            "device capability: 3.7\n",
            "device name: Tesla K80\n"
          ]
        }
      ],
      "source": [
        "# 升级MAESTROv2.0.0 -> 3.0.0 且 输出gpu信息\n",
        "import utils\n",
        "# import preprocessors\n",
        "\n",
        "# preprocessors.upgrade_maestro(r'/content/maestro/')\n",
        "utils.show_gpu_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sY3ZQG0YeqM-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import T5ForConditionalGeneration, T5Config\n",
        "from transformers.optimization import Adafactor, AdafactorSchedule\n",
        "\n",
        "from datasets import MaestroDataset3, MaestroSampler2, collate_fn\n",
        "import vocabularies\n",
        "import config\n",
        "from config.data import YuiConfigPro\n",
        "import utils\n",
        "from train import train, evaluate\n",
        "\n",
        "resume = True\n",
        "\n",
        "\n",
        "# config\n",
        "cf = YuiConfigPro(\n",
        "  DATASET_DIR=r'/content/maestro/',\n",
        "  DATAMETA_NAME=r'maestro-v3.0.0.csv',\n",
        "  WORKSPACE=r'/content/drive/MyDrive/',\n",
        "  # WORKSPACE=r'/content/',\n",
        "  CUDA=True,\n",
        "  NUM_EPOCHS=20,\n",
        "  NUM_WORKERS=2,\n",
        "  BATCH_SIZE=8,\n",
        "  TRAIN_ITERATION=1500,\n",
        ")\n",
        "# 经常分到 k80 显存12GB，batch_size不能太大\n",
        "\n",
        "# Arugments & parameters\n",
        "workspace = cf.WORKSPACE\n",
        "batch_size = cf.BATCH_SIZE\n",
        "device = torch.device('cuda') if cf.CUDA and torch.cuda.is_available() else torch.device('cpu')\n",
        "num_workers = cf.NUM_WORKERS\n",
        "\n",
        "class Adafactor2(Adafactor):\n",
        "  def __init__(\n",
        "    self,\n",
        "    params,\n",
        "    lr=None,\n",
        "    eps=(1e-30, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.8,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    scale_parameter=True,\n",
        "    relative_step=True,\n",
        "    warmup_init=False,\n",
        "  ):\n",
        "    super().__init__(params, lr, eps, clip_threshold, decay_rate, beta1, weight_decay, scale_parameter, relative_step, warmup_init)\n",
        "\n",
        "  @staticmethod\n",
        "  def _get_lr(param_group, param_state):\n",
        "    rel_step_sz = param_group[\"lr\"]\n",
        "    if param_group[\"relative_step\"]:\n",
        "      min_step = 1e-6 * param_state[\"step\"] if param_group[\"warmup_init\"] else 1e-3\n",
        "      exp_lr = math.exp(-(6.45 + param_state[\"step\"] / 3e4))\n",
        "      # 这个值将在step=[1500,30000]从1.5e-3降到9.6e-4\n",
        "      rel_step_sz = min(min_step, exp_lr)\n",
        "    if param_group[\"scale_parameter\"]:\n",
        "      rel_step_sz *= max(param_group[\"eps\"][1], param_state[\"RMS\"])\n",
        "    return rel_step_sz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Z8UbvVvpezGO"
      },
      "outputs": [],
      "source": [
        "# Checkpoint & Log\n",
        "# 单独放置，否则多次创建logger会有多个重复输出\n",
        "\n",
        "checkpoints_dir = os.path.join(workspace, 'checkpoints')\n",
        "utils.create_folder(checkpoints_dir)\n",
        "logs_dir = os.path.join(workspace, 'logs')\n",
        "utils.create_logging(logs_dir, f'train', filemode='w', with_time=True)\n",
        "resume_checkpoint_path = os.path.join(checkpoints_dir, 'model_resume.pt')\n",
        "best_checkpoint_path = os.path.join(checkpoints_dir, 'model_best.pt')\n",
        "statistics_path = os.path.join(checkpoints_dir, 'statistics.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tr4wDeOwhNAn",
        "outputId": "90a3bf37-68ff-46d9-b5d9-cc872ea23724"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "root: INFO YuiConfigPro(RANDOM_SEED=233, DATASET_DIR='/content/maestro/', DATAMETA_NAME='maestro-v3.0.0.csv', WORKSPACE='/content/drive/MyDrive/', MAX_INPUTS_LENGTH=512, MAX_TARGETS_LENGTH=1024, PROGRAM_GRANULARITY='flat', SAMPLE_RATE=16000, FRAME_SIZE=128, HOP_WIDTH=128, NUM_MEL_BINS=384, FFT_SIZE=2048, MEL_LO_HZ=20.0, MEL_HI_HZ=8000.0, PAD_ID=0, ENCODED_EOS_ID=1, ENCODED_UNK_ID=2, EXTRA_IDS=0, DECODED_EOS_ID=-1, DECODED_INVALID_ID=-2, STEPS_PER_SECOND=100, MAX_SHIFT_SECONDS=10, NUM_VELOCITY_BINS=127, CUDA=True, BATCH_SIZE=8, EXPECT_BATCH_SIZE=128, NUM_WORKERS=2, NUM_EPOCHS=20, TRAIN_ITERATION=1500, LEARNING_RATE=0.001, OVERFIT_PATIENCE=8)\n",
            "root: INFO Using GPU.\n",
            "root: INFO GPU number: 1\n"
          ]
        }
      ],
      "source": [
        "# Codec & Vocabulary\n",
        "codec = vocabularies.build_codec(cf)\n",
        "vocabulary = vocabularies.Vocabulary(cf, codec.num_classes, extra_ids=cf.EXTRA_IDS)\n",
        "t5_config_map = config.build_t5_config(\n",
        "  d_model=cf.NUM_MEL_BINS,\n",
        "  vocab_size=vocabulary.vocab_size,\n",
        "  max_length=cf.MAX_TARGETS_LENGTH,\n",
        ")\n",
        "# 简化模型，否则根本训练不动\n",
        "\n",
        "logging.info(cf)  \n",
        "if device.type == 'cuda':\n",
        "  logging.info('Using GPU.')\n",
        "  logging.info(f'GPU number: {torch.cuda.device_count()}')\n",
        "else:\n",
        "  logging.info('Using CPU.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "578hvYuN7_uL"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "# 释放显存的cache，可能要多次执行才有效"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nrtuzpomzRh",
        "outputId": "4feceaed-0ccc-47de-8a58-28ab500cbcff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "root: INFO T5Config {\n",
            "  \"bos_token_id\": 0,\n",
            "  \"d_ff\": 512,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 384,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"forced_eos_token_id\": 1,\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"max_length\": 1024,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_beams\": 4,\n",
            "  \"num_decoder_layers\": 6,\n",
            "  \"num_heads\": 6,\n",
            "  \"num_layers\": 6,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"top_p\": 0.95,\n",
            "  \"transformers_version\": \"4.18.0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 669\n",
            "}\n",
            "\n",
            "root: INFO The model has 18,221,184 trainable parameters\n",
            "root: INFO resume training with epoch=0\n",
            "root: INFO statistics = {'epoch': 0, 'train_loss': [0.4050339981118837, 0.37911855947971357, 0.36502313115199314, 0.35639949337641413, 0.347767071386178], 'eval_loss': []}\n",
            "root: INFO -------train loop starts, start_time=1651299029.068s-------\n",
            "root: INFO -------train starts, epoch=0-------\n",
            "root: INFO train: epoch=0, iteration=50, loss=0.33014652132987976, lr=[tensor(0.0005, device='cuda:0')], in 88.633s\n",
            "root: INFO train: epoch=0, iteration=100, loss=0.32402268052101135, lr=[tensor(0.0005, device='cuda:0')], in 61.909s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1024\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=150, loss=0.3321366012096405, lr=[tensor(0.0005, device='cuda:0')], in 60.246s\n",
            "root: INFO train: epoch=0, iteration=200, loss=0.32831594347953796, lr=[tensor(0.0005, device='cuda:0')], in 58.883s\n",
            "root: INFO train: epoch=0, iteration=250, loss=0.33585140109062195, lr=[tensor(0.0005, device='cuda:0')], in 60.363s\n",
            "root: INFO train: epoch=0, iteration=300, loss=0.32798486948013306, lr=[tensor(0.0005, device='cuda:0')], in 60.173s\n",
            "root: INFO train: epoch=0, iteration=350, loss=0.3243277370929718, lr=[tensor(0.0005, device='cuda:0')], in 65.315s\n",
            "root: INFO train: epoch=0, iteration=400, loss=0.32568344473838806, lr=[tensor(0.0005, device='cuda:0')], in 59.722s\n",
            "root: INFO train: epoch=0, iteration=450, loss=0.3431030213832855, lr=[tensor(0.0005, device='cuda:0')], in 60.182s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=128\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=500, loss=0.33924147486686707, lr=[tensor(0.0005, device='cuda:0')], in 91.077s\n",
            "root: INFO train: epoch=0, iteration=550, loss=0.3235994279384613, lr=[tensor(0.0005, device='cuda:0')], in 83.516s\n",
            "root: INFO train: epoch=0, iteration=600, loss=0.3202615976333618, lr=[tensor(0.0005, device='cuda:0')], in 86.577s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1792\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=650, loss=0.33914196491241455, lr=[tensor(0.0005, device='cuda:0')], in 62.696s\n",
            "root: INFO train: epoch=0, iteration=700, loss=0.34222882986068726, lr=[tensor(0.0005, device='cuda:0')], in 78.848s\n",
            "root: INFO train: epoch=0, iteration=750, loss=0.3270581364631653, lr=[tensor(0.0005, device='cuda:0')], in 65.980s\n",
            "root: INFO train: epoch=0, iteration=800, loss=0.3547878563404083, lr=[tensor(0.0005, device='cuda:0')], in 82.228s\n",
            "root: INFO train: epoch=0, iteration=850, loss=0.34297823905944824, lr=[tensor(0.0005, device='cuda:0')], in 63.033s\n",
            "root: INFO train: epoch=0, iteration=900, loss=0.3246869146823883, lr=[tensor(0.0005, device='cuda:0')], in 59.292s\n",
            "root: INFO train: epoch=0, iteration=950, loss=0.3398774266242981, lr=[tensor(0.0005, device='cuda:0')], in 68.289s\n",
            "root: INFO train: epoch=0, iteration=1000, loss=0.32960909605026245, lr=[tensor(0.0005, device='cuda:0')], in 84.191s\n",
            "root: INFO train: epoch=0, iteration=1050, loss=0.3324097990989685, lr=[tensor(0.0005, device='cuda:0')], in 81.811s\n",
            "root: INFO train: epoch=0, iteration=1100, loss=0.32446062564849854, lr=[tensor(0.0005, device='cuda:0')], in 82.108s\n",
            "root: INFO train: epoch=0, iteration=1150, loss=0.32640862464904785, lr=[tensor(0.0005, device='cuda:0')], in 61.918s\n",
            "root: INFO train: epoch=0, iteration=1200, loss=0.3266897201538086, lr=[tensor(0.0005, device='cuda:0')], in 93.378s\n",
            "root: INFO train: epoch=0, iteration=1250, loss=0.31717729568481445, lr=[tensor(0.0005, device='cuda:0')], in 62.198s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=128\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=1300, loss=0.3127765953540802, lr=[tensor(0.0005, device='cuda:0')], in 59.184s\n",
            "root: INFO train: epoch=0, iteration=1350, loss=0.31670278310775757, lr=[tensor(0.0005, device='cuda:0')], in 61.308s\n",
            "root: INFO train: epoch=0, iteration=1400, loss=0.32049256563186646, lr=[tensor(0.0005, device='cuda:0')], in 66.155s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=640\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=1450, loss=0.3337607681751251, lr=[tensor(0.0006, device='cuda:0')], in 59.299s\n",
            "root: INFO train: epoch=0, iteration=1500, loss=0.3317914307117462, lr=[tensor(0.0006, device='cuda:0')], in 73.441s\n",
            "root: INFO -------train exits, epoch=0-------\n",
            "root: INFO save model and statistics to /content/drive/MyDrive/checkpoints\n",
            "root: INFO -------train starts, epoch=0-------\n",
            "root: INFO train: epoch=0, iteration=50, loss=0.3248153328895569, lr=[tensor(0.0006, device='cuda:0')], in 85.128s\n",
            "root: INFO train: epoch=0, iteration=100, loss=0.3244192600250244, lr=[tensor(0.0006, device='cuda:0')], in 59.498s\n",
            "root: INFO train: epoch=0, iteration=150, loss=0.305800199508667, lr=[tensor(0.0006, device='cuda:0')], in 76.251s\n",
            "root: INFO train: epoch=0, iteration=200, loss=0.32461512088775635, lr=[tensor(0.0006, device='cuda:0')], in 59.276s\n",
            "root: INFO train: epoch=0, iteration=250, loss=0.34039995074272156, lr=[tensor(0.0006, device='cuda:0')], in 87.762s\n",
            "root: INFO train: epoch=0, iteration=300, loss=0.30405452847480774, lr=[tensor(0.0006, device='cuda:0')], in 61.003s\n",
            "root: INFO train: epoch=0, iteration=350, loss=0.33055341243743896, lr=[tensor(0.0006, device='cuda:0')], in 61.943s\n",
            "root: INFO train: epoch=0, iteration=400, loss=0.30378296971321106, lr=[tensor(0.0006, device='cuda:0')], in 67.177s\n",
            "root: INFO train: epoch=0, iteration=450, loss=0.3035094738006592, lr=[tensor(0.0006, device='cuda:0')], in 59.367s\n",
            "root: INFO train: epoch=0, iteration=500, loss=0.3107980191707611, lr=[tensor(0.0006, device='cuda:0')], in 59.154s\n",
            "root: INFO train: epoch=0, iteration=550, loss=0.3566591441631317, lr=[tensor(0.0006, device='cuda:0')], in 73.441s\n",
            "root: INFO train: epoch=0, iteration=600, loss=0.32467105984687805, lr=[tensor(0.0006, device='cuda:0')], in 78.272s\n",
            "root: INFO train: epoch=0, iteration=650, loss=0.3025504946708679, lr=[tensor(0.0006, device='cuda:0')], in 72.065s\n",
            "root: INFO train: epoch=0, iteration=700, loss=0.30218151211738586, lr=[tensor(0.0006, device='cuda:0')], in 90.892s\n",
            "root: INFO train: epoch=0, iteration=750, loss=0.3037050664424896, lr=[tensor(0.0006, device='cuda:0')], in 71.356s\n",
            "root: INFO train: epoch=0, iteration=800, loss=0.3356550335884094, lr=[tensor(0.0006, device='cuda:0')], in 59.329s\n",
            "root: INFO train: epoch=0, iteration=850, loss=0.29622259736061096, lr=[tensor(0.0006, device='cuda:0')], in 59.113s\n",
            "root: INFO train: epoch=0, iteration=900, loss=0.2978655993938446, lr=[tensor(0.0006, device='cuda:0')], in 59.184s\n",
            "root: INFO train: epoch=0, iteration=950, loss=0.3033793270587921, lr=[tensor(0.0006, device='cuda:0')], in 59.140s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1024\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=1000, loss=0.29999664425849915, lr=[tensor(0.0006, device='cuda:0')], in 59.533s\n",
            "root: INFO train: epoch=0, iteration=1050, loss=0.3236600458621979, lr=[tensor(0.0006, device='cuda:0')], in 70.721s\n",
            "root: INFO train: epoch=0, iteration=1100, loss=0.3013499081134796, lr=[tensor(0.0006, device='cuda:0')], in 58.976s\n",
            "root: INFO train: epoch=0, iteration=1150, loss=0.3046710193157196, lr=[tensor(0.0006, device='cuda:0')], in 69.688s\n",
            "root: INFO train: epoch=0, iteration=1200, loss=0.33485931158065796, lr=[tensor(0.0006, device='cuda:0')], in 59.557s\n",
            "root: INFO train: epoch=0, iteration=1250, loss=0.3329741060733795, lr=[tensor(0.0006, device='cuda:0')], in 59.357s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1920\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=1300, loss=0.3013975918292999, lr=[tensor(0.0006, device='cuda:0')], in 59.087s\n",
            "root: INFO train: epoch=0, iteration=1350, loss=0.3115108907222748, lr=[tensor(0.0006, device='cuda:0')], in 59.135s\n",
            "root: INFO train: epoch=0, iteration=1400, loss=0.3076580762863159, lr=[tensor(0.0006, device='cuda:0')], in 59.236s\n",
            "root: INFO train: epoch=0, iteration=1450, loss=0.2853475511074066, lr=[tensor(0.0006, device='cuda:0')], in 59.468s\n",
            "root: INFO train: epoch=0, iteration=1500, loss=0.3384205102920532, lr=[tensor(0.0006, device='cuda:0')], in 84.403s\n",
            "root: INFO -------train exits, epoch=0-------\n",
            "root: INFO save model and statistics to /content/drive/MyDrive/checkpoints\n",
            "root: INFO -------train starts, epoch=0-------\n",
            "root: INFO train: epoch=0, iteration=50, loss=0.30551809072494507, lr=[tensor(0.0006, device='cuda:0')], in 91.112s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=1792\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=100, loss=0.3046473264694214, lr=[tensor(0.0007, device='cuda:0')], in 59.313s\n",
            "root: INFO train: epoch=0, iteration=150, loss=0.29069051146507263, lr=[tensor(0.0007, device='cuda:0')], in 59.324s\n",
            "root: INFO train: epoch=0, iteration=200, loss=0.3151293396949768, lr=[tensor(0.0007, device='cuda:0')], in 58.933s\n",
            "root: INFO train: epoch=0, iteration=250, loss=0.2913794219493866, lr=[tensor(0.0007, device='cuda:0')], in 59.101s\n",
            "root: INFO train: epoch=0, iteration=300, loss=0.2859804630279541, lr=[tensor(0.0007, device='cuda:0')], in 59.041s\n",
            "root: INFO train: epoch=0, iteration=350, loss=0.2869855761528015, lr=[tensor(0.0007, device='cuda:0')], in 59.153s\n",
            "root: INFO train: epoch=0, iteration=400, loss=0.28579843044281006, lr=[tensor(0.0007, device='cuda:0')], in 59.395s\n",
            "root: INFO train: epoch=0, iteration=450, loss=0.30704769492149353, lr=[tensor(0.0007, device='cuda:0')], in 59.355s\n",
            "root: INFO train: epoch=0, iteration=500, loss=0.32093748450279236, lr=[tensor(0.0007, device='cuda:0')], in 62.687s\n",
            "root: INFO train: epoch=0, iteration=550, loss=0.30038538575172424, lr=[tensor(0.0007, device='cuda:0')], in 73.366s\n",
            "root: INFO train: epoch=0, iteration=600, loss=0.30431681871414185, lr=[tensor(0.0007, device='cuda:0')], in 85.026s\n",
            "root: INFO train: epoch=0, iteration=650, loss=0.3226183354854584, lr=[tensor(0.0007, device='cuda:0')], in 59.214s\n",
            "root: INFO train: epoch=0, iteration=700, loss=0.2911847233772278, lr=[tensor(0.0007, device='cuda:0')], in 72.063s\n",
            "root: INFO train: epoch=0, iteration=750, loss=0.3051953911781311, lr=[tensor(0.0007, device='cuda:0')], in 63.962s\n",
            "root: INFO train: epoch=0, iteration=800, loss=0.29332253336906433, lr=[tensor(0.0007, device='cuda:0')], in 59.336s\n",
            "root: INFO train: epoch=0, iteration=850, loss=0.3160876929759979, lr=[tensor(0.0007, device='cuda:0')], in 59.168s\n",
            "root: INFO train: epoch=0, iteration=900, loss=0.30473530292510986, lr=[tensor(0.0007, device='cuda:0')], in 72.664s\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=768\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=950, loss=0.2920690178871155, lr=[tensor(0.0007, device='cuda:0')], in 63.605s\n",
            "root: INFO train: epoch=0, iteration=1000, loss=0.29535526037216187, lr=[tensor(0.0007, device='cuda:0')], in 73.861s\n",
            "root: INFO train: epoch=0, iteration=1050, loss=0.28727737069129944, lr=[tensor(0.0007, device='cuda:0')], in 66.687s\n",
            "root: INFO train: epoch=0, iteration=1100, loss=0.29430437088012695, lr=[tensor(0.0007, device='cuda:0')], in 59.437s\n",
            "root: INFO train: epoch=0, iteration=1150, loss=0.3091472089290619, lr=[tensor(0.0007, device='cuda:0')], in 61.304s\n",
            "root: INFO train: epoch=0, iteration=1200, loss=0.2864181697368622, lr=[tensor(0.0007, device='cuda:0')], in 59.148s\n",
            "root: INFO train: epoch=0, iteration=1250, loss=0.2912386655807495, lr=[tensor(0.0007, device='cuda:0')], in 59.191s\n",
            "root: INFO train: epoch=0, iteration=1300, loss=0.298230916261673, lr=[tensor(0.0007, device='cuda:0')], in 59.401s\n",
            "root: INFO train: epoch=0, iteration=1350, loss=0.27576136589050293, lr=[tensor(0.0007, device='cuda:0')], in 59.954s\n",
            "root: INFO train: epoch=0, iteration=1400, loss=0.29906022548675537, lr=[tensor(0.0007, device='cuda:0')], in 62.870s\n",
            "root: INFO train: epoch=0, iteration=1450, loss=0.31128644943237305, lr=[tensor(0.0007, device='cuda:0')], in 62.177s\n",
            "root: INFO train: epoch=0, iteration=1500, loss=0.27946770191192627, lr=[tensor(0.0007, device='cuda:0')], in 78.964s\n",
            "root: INFO -------train exits, epoch=0-------\n",
            "root: INFO save model and statistics to /content/drive/MyDrive/checkpoints\n",
            "root: INFO -------train starts, epoch=0-------\n",
            "/usr/local/lib/python3.7/dist-packages/librosa/core/spectrum.py:224: UserWarning: n_fft=2048 is too small for input signal of length=512\n",
            "  n_fft, y.shape[-1]\n",
            "root: INFO train: epoch=0, iteration=50, loss=0.2987218499183655, lr=[tensor(0.0007, device='cuda:0')], in 76.513s\n",
            "root: INFO train: epoch=0, iteration=100, loss=0.3035525381565094, lr=[tensor(0.0007, device='cuda:0')], in 69.837s\n",
            "root: INFO train: epoch=0, iteration=150, loss=0.3025094270706177, lr=[tensor(0.0007, device='cuda:0')], in 58.631s\n",
            "root: INFO train: epoch=0, iteration=200, loss=0.31674930453300476, lr=[tensor(0.0007, device='cuda:0')], in 58.752s\n",
            "root: INFO train: epoch=0, iteration=250, loss=0.2722870111465454, lr=[tensor(0.0008, device='cuda:0')], in 63.167s\n",
            "root: INFO train: epoch=0, iteration=300, loss=0.3083610534667969, lr=[tensor(0.0008, device='cuda:0')], in 89.640s\n",
            "root: INFO train: epoch=0, iteration=350, loss=0.29123538732528687, lr=[tensor(0.0008, device='cuda:0')], in 72.735s\n",
            "root: INFO train: epoch=0, iteration=400, loss=0.2993728220462799, lr=[tensor(0.0008, device='cuda:0')], in 60.203s\n",
            "root: INFO train: epoch=0, iteration=450, loss=0.29511764645576477, lr=[tensor(0.0008, device='cuda:0')], in 59.898s\n",
            "root: INFO train: epoch=0, iteration=500, loss=0.28925976157188416, lr=[tensor(0.0008, device='cuda:0')], in 69.446s\n",
            "root: INFO train: epoch=0, iteration=550, loss=0.29126420617103577, lr=[tensor(0.0008, device='cuda:0')], in 77.672s\n",
            "root: INFO train: epoch=0, iteration=600, loss=0.29600611329078674, lr=[tensor(0.0008, device='cuda:0')], in 59.037s\n",
            "root: INFO train: epoch=0, iteration=650, loss=0.2928679883480072, lr=[tensor(0.0008, device='cuda:0')], in 59.345s\n",
            "root: INFO train: epoch=0, iteration=700, loss=0.2909303307533264, lr=[tensor(0.0008, device='cuda:0')], in 58.930s\n",
            "root: INFO train: epoch=0, iteration=750, loss=0.3137553334236145, lr=[tensor(0.0008, device='cuda:0')], in 59.029s\n",
            "root: INFO train: epoch=0, iteration=800, loss=0.29505982995033264, lr=[tensor(0.0008, device='cuda:0')], in 59.300s\n",
            "root: INFO train: epoch=0, iteration=850, loss=0.28030136227607727, lr=[tensor(0.0008, device='cuda:0')], in 58.676s\n",
            "root: INFO train: epoch=0, iteration=900, loss=0.27049675583839417, lr=[tensor(0.0008, device='cuda:0')], in 58.406s\n",
            "root: INFO train: epoch=0, iteration=950, loss=0.2780267298221588, lr=[tensor(0.0008, device='cuda:0')], in 89.460s\n",
            "root: INFO train: epoch=0, iteration=1000, loss=0.2987649738788605, lr=[tensor(0.0008, device='cuda:0')], in 83.999s\n",
            "root: INFO train: epoch=0, iteration=1050, loss=0.2815944254398346, lr=[tensor(0.0008, device='cuda:0')], in 73.880s\n",
            "root: INFO train: epoch=0, iteration=1100, loss=0.30605995655059814, lr=[tensor(0.0008, device='cuda:0')], in 69.658s\n",
            "root: INFO train: epoch=0, iteration=1150, loss=0.3098421096801758, lr=[tensor(0.0008, device='cuda:0')], in 58.965s\n"
          ]
        }
      ],
      "source": [
        "# Dataset\n",
        "meta_path = os.path.join(cf.DATASET_DIR, cf.DATAMETA_NAME)\n",
        "\n",
        "train_sampler = MaestroSampler2(meta_path, 'train', batch_size=batch_size, config=cf, max_iter_num=cf.TRAIN_ITERATION)\n",
        "train_dataset = MaestroDataset3(cf.DATASET_DIR, cf, codec, vocabulary, meta_file=cf.DATAMETA_NAME)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_sampler=train_sampler, collate_fn=collate_fn, num_workers=num_workers, pin_memory=True)\n",
        "\n",
        "validate_sampler = MaestroSampler2(meta_path, 'validation', batch_size=batch_size, config=cf, max_iter_num=-1)\n",
        "validate_loader = DataLoader(dataset=train_dataset, batch_sampler=validate_sampler, collate_fn=collate_fn, num_workers=num_workers, pin_memory=True)\n",
        "# pin_memory: 锁页内存，不会与虚存进行交换，转到gpu时快一些，但很容易超出gpu显存\n",
        "\n",
        "# Model\n",
        "t5_config = T5Config.from_dict(t5_config_map)\n",
        "logging.info(t5_config)\n",
        "model = T5ForConditionalGeneration(config=t5_config)\n",
        "logging.info(f'The model has {model.num_parameters():,} trainable parameters')\n",
        "# 17,896 for dev; 48,626,048 for pro; while T5-Small has 60 million parameters\n",
        "\n",
        "# Early stop\n",
        "early_stopping = utils.EarlyStopping(\n",
        "  best_path=best_checkpoint_path,\n",
        "  resume_path=resume_checkpoint_path,\n",
        "  patience=cf.OVERFIT_PATIENCE, \n",
        "  verbose=True\n",
        ")\n",
        "\n",
        "# Resume training\n",
        "resume_epoch = 0\n",
        "learning_rate = cf.LEARNING_RATE\n",
        "statistics = {\n",
        "  'epoch': 0,\n",
        "  'train_loss': [],\n",
        "  'eval_loss': []\n",
        "}\n",
        "\n",
        "# Loss function\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=cf.PAD_ID)\n",
        "\n",
        "# Optimizer\n",
        "# optimizer = Adafactor(model.parameters(), lr=learning_rate, scale_parameter=False, relative_step=False, warmup_init=False)\n",
        "optimizer = Adafactor2(model.parameters(), scale_parameter=True, relative_step=True, warmup_init=True, lr=None)\n",
        "scheduler = AdafactorSchedule(optimizer, learning_rate)\n",
        "\n",
        "if not resume:\n",
        "  ...\n",
        "  # 从头开始训练模型\n",
        "elif not os.path.isfile(resume_checkpoint_path):\n",
        "  logging.info(f'resume_checkpoint_path={resume_checkpoint_path} does not exist, train from scratch')\n",
        "elif not os.path.isfile(statistics_path):\n",
        "  logging.info(f'statistics_path={statistics_path} does not exist, train from scratch')\n",
        "else:\n",
        "  statistics = torch.load(statistics_path)\n",
        "  # 单独保存后面数据分析读取方便些\n",
        "  # raise FileNotFoundError(f'resume_checkpoint_path={resume_checkpoint_path} does not exist')\n",
        "  checkpoint = torch.load(resume_checkpoint_path)\n",
        "  # 以TRAIN_ITERATION为单位保存checkpoint\n",
        "  early_stopping.load_state_dict(checkpoint['early_stopping'])\n",
        "\n",
        "  model.load_state_dict(checkpoint['model'])\n",
        "  train_sampler.load_state_dict(checkpoint['sampler'])\n",
        "  validate_sampler.epoch = train_sampler.epoch\n",
        "  # 二者epoch一致\n",
        "  resume_epoch = checkpoint['epoch']\n",
        "  # scheduler.get_lr 拿到的lr是个列表\n",
        "  optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "  logging.info(f'resume training with epoch={resume_epoch}')\n",
        "  logging.info(f'statistics = {statistics}')\n",
        "\n",
        "model.to(device)\n",
        "epoch = resume_epoch\n",
        "loop_start_time = time.time()\n",
        "start_time = time.time()\n",
        "logging.info(f'-------train loop starts, start_time={start_time:.3f}s-------')\n",
        "\n",
        "# for epoch in range(resume_epoch, cf.NUM_EPOCHS):\n",
        "while epoch < cf.NUM_EPOCHS:\n",
        "  train_loss = train(model, device, train_loader, criterion, optimizer, scheduler, accumulation_steps=cf.accumulation_steps)\n",
        "  statistics['train_loss'].append(train_loss)\n",
        "  current_lr = scheduler.get_lr()\n",
        "\n",
        "  # 训练数据完整采样一轮\n",
        "  if train_sampler.epoch > epoch:\n",
        "    validate_sampler.reset_state()\n",
        "    validate_loss = evaluate(model, device, validate_loader, criterion)\n",
        "    statistics['eval_loss'].append(validate_loss)\n",
        "    # 等train数据完整过了一遍再进行评估\n",
        "    logging.info(\n",
        "      f'epoch={epoch} finish, time={time.time()-start_time:.3f}s, train_loss={train_loss}, validate_loss={validate_loss}'\n",
        "      f', with lr={current_lr}'\n",
        "    )\n",
        "\n",
        "    early_stopping(validate_loss)\n",
        "    if early_stopping.stop:\n",
        "      logging.info(f'early stoping')\n",
        "      break\n",
        "\n",
        "    epoch += 1\n",
        "    start_time = time.time()\n",
        "    train_sampler.reset_state()\n",
        "  \n",
        "  # Save model\n",
        "  statistics['epoch'] = epoch\n",
        "  checkpoint = {\n",
        "    'epoch': epoch,\n",
        "    'model': model.state_dict(),\n",
        "    'sampler': train_sampler.state_dict(),\n",
        "    'early_stopping': early_stopping.state_dict(),\n",
        "    'optimizer': optimizer.state_dict(),\n",
        "  }\n",
        "  torch.save(checkpoint, resume_checkpoint_path)\n",
        "  torch.save(statistics, statistics_path)\n",
        "  logging.info(f'save model and statistics to {checkpoints_dir}')\n",
        "logging.info(f'-------train loop ends, time={time.time()-loop_start_time:.3f}s-------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMMhClwUrF-O"
      },
      "outputs": [],
      "source": [
        "!rm /content/logs -r"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "yui_colab.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "d5c8e674f5dfa726cbc4d7a25209bb75f43fdf268afb841d4cc164f7f4d4aeee"
    },
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
